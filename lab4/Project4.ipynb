{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Steven Koprowicz, \n",
    " > Matthew Walters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=64, rate=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        # Use the rate variable for the dropout layers\n",
    "        \n",
    "        # MultiHeadAttention layer, \n",
    "        # specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim, use_causal_mask=True)(inputs,inputs)\n",
    "         \n",
    "        # drop-out layer\n",
    "        first_dropout = layers.Dropout(self.rate)(mha)\n",
    "        \n",
    "        # sum inputs and the output of this drop-out layer\n",
    "        first_sum = inputs + first_dropout\n",
    "\n",
    "        # LayerNormalization layer, \n",
    "        # specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        first_ln = layers.LayerNormalization(epsilon=1e-6)(first_sum)\n",
    "        \n",
    "        # dense\n",
    "        first_dense = layers.Dense(self.ff_dim, activation=\"relu\")(first_ln)\n",
    "        \n",
    "        # dense\n",
    "        second_dense = layers.Dense(self.embed_dim)(first_dense)\n",
    "\n",
    "        # drop-out layer\n",
    "        second_dropout = layers.Dropout(self.rate)(second_dense)\n",
    "\n",
    "        # sum the output of the first Layer Normalization layer and this drop-out layer\n",
    "        second_sum = first_ln + second_dropout\n",
    "\n",
    "        # LayerNormalization again\n",
    "        second_ln = layers.LayerNormalization(epsilon=1e-6)(second_sum)\n",
    "\n",
    "        return second_ln\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to encode positions\n",
    "        positions = tf.range(maxlen)\n",
    "        tokens_embed = layers.Embedding(self.vocab_size, self.embed_dim)\n",
    "        positions_embed = layers.Embedding(self.maxlen, self.embed_dim)\n",
    "        \n",
    "        #add (1) and (2) and return the layer\n",
    "        embedding_output = tokens_embed(inputs) + positions_embed(positions)\n",
    "        return embedding_output\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function\n",
    "        # (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "        inputs = keras.Input(shape=(tf.shape(inputs)[-1]))\n",
    "        embedLayer = self.EmbeddingLayer(inputs)\n",
    "        xBlock = self.TransformerBlock(embedLayer)\n",
    "        for i in range(self.num_blocks-1):\n",
    "            xBlock = self.TransformerBlock(xBlock)\n",
    "        outputBlock = layers.Dense(self.vocab_size, activation=\"softmax\")(xBlock)\n",
    "        self.model = keras.Model(inputs, outputBlock)\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "227111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.filename=filename\n",
    "        self.len = len\n",
    "        with open(filename, \"r\") as f:\n",
    "            self.data = f.readlines()\n",
    "        \n",
    "    def prep_text(self):\n",
    "        #remove all punctuation, set to lowercase, remove duplicate spaces and other whitespace (keep newlines)\n",
    "        self.data = [re.sub(\"--\",\" \",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\n\",\" \\n$\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"((^|(?<=[^\\\\S\\n]))[^\\\\S\\n]+(?!$)|[^\\\\S\\n]+(?= \\\\n?$))\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\\\s+$\",\" \\n\",line) for line in self.data if (re.search(\"[A-Za-z0-9]\",line) is not None)]\n",
    "        return self.data\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number number\n",
    "        vocab_lines = [re.split(\" +\",line) for line in self.data]\n",
    "        self.words = []\n",
    "        for vl in vocab_lines:\n",
    "            self.words+=vl\n",
    "        vocab_list = np.unique(self.words)\n",
    "        self.vocabulary = {word:number for (word,number) in zip(vocab_list,range(len(vocab_list)))}\n",
    "        self.integers = [self.vocabulary[word] for word in self.words]\n",
    "        return self.integers\n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        if self.len==0 or self.len>len(self.integers):\n",
    "           return (self.x, self.y, self.vocabulary)\n",
    "        # this loop makes (#tokens - len + 1) sequences (1 is removed because of the offset)\n",
    "        for i in range(len(self.integers)-self.len+1):\n",
    "            self.x.append(self.integers[i:(i+self.len)])\n",
    "        self.y = self.x[1:]\n",
    "        self.x = self.x[:-1]\n",
    "        # this loop makes floor(#tokens / len) sequences\n",
    "        # for i in range((len(self.integers)-1)//self.len):\n",
    "        #     self.x.append(self.integers[(self.len*i):(self.len*(i+1))])\n",
    "        #     self.y.append(self.integers[(self.len*i+1):(self.len*(i+1)+1)])\n",
    "        return (self.x, self.y, self.vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        # vocab is a dictionary and thus already is our map from integers to human words\n",
    "        self.words_to_ints = vocab\n",
    "        self.ints_to_words = {v: k for k, v in vocab.items()}\n",
    "        self.vocabulary_list = vocab.keys()\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        text_out = []\n",
    "        text_out.append(start_string)\n",
    "        start_string = start_string\n",
    "        start_string = re.sub(\"--\",\" \",start_string)\n",
    "        start_string = re.sub(\"\\n\",\" \\n \",start_string)\n",
    "        start_string = re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",start_string)\n",
    "        start_string = re.sub(\"[^\\\\S\\n]+\",\" \",start_string)\n",
    "        start_string = re.sub(\"([^\\\\S\\n]+$|^[^\\\\S\\n]+)\",\"\",start_string)\n",
    "        output_string = start_string\n",
    "        \n",
    "        start_text_seq = re.split(\"[^\\\\S\\n]+\",start_string)\n",
    "        output_int_seq = [self.words_to_ints[word] if word in self.vocabulary_list else -1 for word in start_text_seq]\n",
    "        \n",
    "        for i in range(num_generate):\n",
    "            next_int_word = self.model.predict(output_int_seq)\n",
    "            output_string = output_string + \" \" + self.ints_to_words[next_int_word]\n",
    "\n",
    "        return(output_string)\n",
    "\n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        text_out = []\n",
    "        starter = self.vocabulary_list[np.random.randint(low=0,high=len(self.vocabulary_list))]\n",
    "        text_out = generate_text(starter, num_generate)\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'inputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [154]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m x,y,vocab_map \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mcreate_dataset()\n\u001b[0;32m      7\u001b[0m tmodel_object \u001b[38;5;241m=\u001b[39m TransformerModel(\u001b[38;5;28mlen\u001b[39m(vocab_map\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtmodel_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m     12\u001b[0m generator  \u001b[38;5;241m=\u001b[39m GenerateText(model,vocab_map)\n",
      "Input \u001b[1;32mIn [151]\u001b[0m, in \u001b[0;36mTransformerModel.create_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m#combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, \u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# use the Keras functional API (https://keras.io/guides/functional_api/)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m#use the SparseCategoricalCrossentropy loss function\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(\u001b[43minputs\u001b[49m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     66\u001b[0m     embedLayer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEmbeddingLayer(inputs)\n\u001b[0;32m     67\u001b[0m     xBlock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransformerBlock(embedLayer)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'inputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50):\n",
    "    return model    \n",
    "\n",
    "ds = DataSet(\"beatles.txt\",13)\n",
    "x,y,vocab_map = ds.create_dataset()\n",
    "tmodel_object = TransformerModel(len(vocab_map.keys()))\n",
    "model = tmodel_object.create_model()\n",
    "\n",
    "print(model)\n",
    "\n",
    "generator  = GenerateText(model,vocab_map)\n",
    "generator.generate_text(\"hungry eyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5537ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b21ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb59a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faceae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ddc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "##### In this project, we set out to construct a transformer model to generate text. The model is trained on the lyrics of Beatles songs. The song lyrics have been stripped of all punctuation, but keep individual spaces between words, and newlines are kept in the text so that the model might generate newlines in the text that it creates. In the end, the model should be able to generate rudimentary lyrics of fictitious Beatles music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "We used only the libraries provided at the top of this file, and our tf version is \n",
    "2.11.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
