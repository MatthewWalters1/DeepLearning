{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Steven Koprowicz, \n",
    " > Matthew Walters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2493f996",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (4086252455.py, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[213], line 65\u001b[1;36m\u001b[0m\n\u001b[1;33m    inputs = layers.Input(shape=[]))\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=64, rate=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        # Use the rate variable for the dropout layers\n",
    "        \n",
    "        # MultiHeadAttention layer, \n",
    "        # specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(inputs,inputs, use_causal_mask=True)\n",
    "         \n",
    "        # drop-out layer\n",
    "        first_dropout = layers.Dropout(self.rate)(mha)\n",
    "        \n",
    "        # sum inputs and the output of this drop-out layer\n",
    "        first_sum = inputs + first_dropout\n",
    "\n",
    "        # LayerNormalization layer, \n",
    "        # specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        first_ln = layers.LayerNormalization(epsilon=1e-6)(first_sum)\n",
    "        \n",
    "        # dense\n",
    "        first_dense = layers.Dense(self.ff_dim, activation=\"relu\")(first_ln)\n",
    "        \n",
    "        # dense\n",
    "        second_dense = layers.Dense(self.embed_dim)(first_dense)\n",
    "\n",
    "        # drop-out layer\n",
    "        second_dropout = layers.Dropout(self.rate)(second_dense)\n",
    "\n",
    "        # sum the output of the first Layer Normalization layer and this drop-out layer\n",
    "        second_sum = first_ln + second_dropout\n",
    "\n",
    "        # LayerNormalization again\n",
    "        second_ln = layers.LayerNormalization(epsilon=1e-6)(second_sum)\n",
    "\n",
    "        return second_ln\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to encode positions\n",
    "        positions = tf.range(self.maxlen)\n",
    "        tokens_embed = layers.Embedding(self.vocab_size, self.embed_dim)\n",
    "        positions_embed = layers.Embedding(self.maxlen, self.embed_dim)\n",
    "        \n",
    "        #add (1) and (2) and return the layer\n",
    "        embedding_output = tokens_embed(inputs) + positions_embed(positions)\n",
    "        return embedding_output\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function\n",
    "        # (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "        inputs = layers.Input(shape=[]))\n",
    "        embedLayer = self.EmbeddingLayer(inputs)\n",
    "        xBlock = self.TransformerBlock(embedLayer)\n",
    "        for i in range(self.num_blocks-1):\n",
    "            xBlock = self.TransformerBlock(xBlock)\n",
    "        outputBlock = layers.Dense(self.vocab_size, activation=\"softmax\")(xBlock)\n",
    "        self.model = keras.Model(inputs, outputBlock)\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "227111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.filename=filename\n",
    "        self.len = len\n",
    "        with open(filename, \"r\") as f:\n",
    "            self.data = f.readlines()\n",
    "        \n",
    "    def prep_text(self):\n",
    "        #remove all punctuation, set to lowercase, remove duplicate spaces and other whitespace (keep newlines)\n",
    "        self.data = [re.sub(\"--\",\" \",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\n\",\" \\n$\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"((^|(?<=[^\\\\S\\n]))[^\\\\S\\n]+(?!$)|[^\\\\S\\n]+(?= \\\\n?$))\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\\\s+$\",\" \\n\",line) for line in self.data if (re.search(\"[A-Za-z0-9]\",line) is not None)]\n",
    "        return self.data\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number number\n",
    "        vocab_lines = [re.split(\" +\",line) for line in self.data]\n",
    "        self.words = []\n",
    "        for vl in vocab_lines:\n",
    "            self.words+=vl\n",
    "        vocab_list = np.unique(self.words)\n",
    "        self.vocabulary = {word:number for (word,number) in zip(vocab_list,range(len(vocab_list)))}\n",
    "        self.integers = [self.vocabulary[word] for word in self.words]\n",
    "        return self.integers\n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        if self.len==0 or self.len>len(self.integers):\n",
    "           return (self.x, self.y, self.vocabulary)\n",
    "        # this loop makes (#tokens - len + 1) sequences (1 is removed because of the offset)\n",
    "        for i in range(len(self.integers)-self.len+1):\n",
    "            self.x.append(self.integers[i:(i+self.len)])\n",
    "        self.y = self.x[1:]\n",
    "        self.x = self.x[:-1]\n",
    "        # this loop makes floor(#tokens / len) sequences\n",
    "        # for i in range((len(self.integers)-1)//self.len):\n",
    "        #     self.x.append(self.integers[(self.len*i):(self.len*(i+1))])\n",
    "        #     self.y.append(self.integers[(self.len*i+1):(self.len*(i+1)+1)])\n",
    "        return (self.x, self.y, self.vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        # vocab is a dictionary and thus already is our map from integers to human words\n",
    "        self.words_to_ints = vocab\n",
    "        self.ints_to_words = {v: k for k, v in vocab.items()}\n",
    "        self.vocabulary_list = vocab.keys()\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        text_out = []\n",
    "        text_out.append(start_string)\n",
    "        start_string = start_string\n",
    "        start_string = re.sub(\"--\",\" \",start_string)\n",
    "        start_string = re.sub(\"\\n\",\" \\n \",start_string)\n",
    "        start_string = re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",start_string)\n",
    "        start_string = re.sub(\"[^\\\\S\\n]+\",\" \",start_string)\n",
    "        start_string = re.sub(\"([^\\\\S\\n]+$|^[^\\\\S\\n]+)\",\"\",start_string)\n",
    "        output_string = start_string\n",
    "        \n",
    "        start_text_seq = re.split(\"[^\\\\S\\n]+\",start_string)\n",
    "        output_int_seq = [self.words_to_ints[word] if word in self.vocabulary_list else -1 for word in start_text_seq]\n",
    "        print(start_text_seq)\n",
    "        print(output_int_seq)\n",
    "        output_int_seq = keras.preprocessing.sequence.pad_sequences([output_int_seq],tmodel_object.maxlen)\n",
    "        #for i in range(num_generate):\n",
    "        next_int_word = self.model.predict(output_int_seq)[0]\n",
    "        \n",
    "        print(next_int_word)\n",
    "        print(len(next_int_word))\n",
    "        print(len(next_int_word[0][0]))\n",
    "        print(len(next_int_word[0]))\n",
    "\n",
    "        output_string = output_string + \" \" + self.ints_to_words[next_int_word]\n",
    "\n",
    "        return(output_string)\n",
    "\n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        text_out = []\n",
    "        starter = self.vocabulary_list[np.random.randint(low=0,high=len(self.vocabulary_list))]\n",
    "        text_out = generate_text(starter, num_generate)\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_43\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_43 (TFOpLam  (2,)                0           ['input_45[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_43 (S  ()                  0           ['tf.compat.v1.shape_43[0][0]']  \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.range_43 (TFOpLambda)       (64,)                0           ['tf.__operators__.getitem_43[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_86 (Embedding)       (None, 64, 256)      662784      ['input_45[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_87 (Embedding)       (64, 256)            16384       ['tf.range_43[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_129 (TFOp  (None, 64, 256)     0           ['embedding_86[0][0]',           \n",
      " Lambda)                                                          'embedding_87[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_43 (Multi  (None, 64, 256)     526080      ['tf.__operators__.add_129[0][0]'\n",
      " HeadAttention)                                                  , 'tf.__operators__.add_129[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)           (None, 64, 256)      0           ['multi_head_attention_43[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_130 (TFOp  (None, 64, 256)     0           ['tf.__operators__.add_129[0][0]'\n",
      " Lambda)                                                         , 'dropout_86[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_86 (LayerN  (None, 64, 256)     512         ['tf.__operators__.add_130[0][0]'\n",
      " ormalization)                                                   ]                                \n",
      "                                                                                                  \n",
      " dense_129 (Dense)              (None, 64, 256)      65792       ['layer_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 64, 256)      65792       ['dense_129[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_87 (Dropout)           (None, 64, 256)      0           ['dense_130[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_131 (TFOp  (None, 64, 256)     0           ['layer_normalization_86[0][0]', \n",
      " Lambda)                                                          'dropout_87[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_87 (LayerN  (None, 64, 256)     512         ['tf.__operators__.add_131[0][0]'\n",
      " ormalization)                                                   ]                                \n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 64, 2589)     665373      ['layer_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,003,229\n",
      "Trainable params: 2,003,229\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "['yesterday']\n",
      "[2569]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "[[0.00021935 0.00027874 0.00034796 ... 0.00041197 0.00055181 0.00041507]\n",
      " [0.00031545 0.00019695 0.0004115  ... 0.00028804 0.00052069 0.00058428]\n",
      " [0.0002065  0.0005184  0.00031071 ... 0.0003333  0.0002234  0.00036296]\n",
      " ...\n",
      " [0.00019988 0.00029085 0.00036023 ... 0.00021198 0.00031926 0.00084461]\n",
      " [0.0001956  0.00024033 0.00024414 ... 0.00017428 0.00031484 0.00060909]\n",
      " [0.00024059 0.00065088 0.00042005 ... 0.00043624 0.00061312 0.00045829]]\n",
      "64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[248], line 22\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# w = model.fit(\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m#     x, y, batch_size=32, epochs=1, validation_data=(x, y)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# print(a)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# model.predict(a[0])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m generator  \u001b[39m=\u001b[39m GenerateText(model,vocab_map)\n\u001b[1;32m---> 22\u001b[0m generator\u001b[39m.\u001b[39;49mgenerate_text(\u001b[39m\"\u001b[39;49m\u001b[39myesterday\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[246], line 31\u001b[0m, in \u001b[0;36mGenerateText.generate_text\u001b[1;34m(self, start_string, num_generate)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(next_int_word)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(next_int_word))\n\u001b[1;32m---> 31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39;49m(next_int_word[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(next_int_word[\u001b[39m0\u001b[39m]))\n\u001b[0;32m     34\u001b[0m output_string \u001b[39m=\u001b[39m output_string \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mints_to_words[next_int_word]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
     ]
    }
   ],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50):\n",
    "    return model    \n",
    "\n",
    "\n",
    "ds = DataSet(\"beatles.txt\",8)\n",
    "x,y,vocab_map = ds.create_dataset()\n",
    "tmodel_object = TransformerModel(len(vocab_map.keys()))\n",
    "x = keras.preprocessing.sequence.pad_sequences(x[1:100],tmodel_object.maxlen)\n",
    "y = keras.preprocessing.sequence.pad_sequences(y[1:100],tmodel_object.maxlen)\n",
    "model = tmodel_object.create_model()\n",
    "print(model.summary())\n",
    "# w = model.fit(\n",
    "#     x, y, batch_size=32, epochs=1, validation_data=(x, y)\n",
    "# )\n",
    "\n",
    "# a = keras.preprocessing.sequence.pad_sequences([[259,3]],tmodel_object.maxlen)\n",
    "# print(a)\n",
    "# model.predict(a[0])\n",
    "\n",
    "generator  = GenerateText(model,vocab_map)\n",
    "generator.generate_text(\"yesterday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "##### In this project, we set out to construct a transformer model to generate text. The model is trained on the lyrics of Beatles songs. The song lyrics have been stripped of all punctuation, but keep individual spaces between words, and newlines are kept in the text so that the model might generate newlines in the text that it creates. In the end, the model should be able to generate rudimentary lyrics of fictitious Beatles music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "We used only the libraries provided at the top of this file, and our tf version is \n",
    "2.11.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
