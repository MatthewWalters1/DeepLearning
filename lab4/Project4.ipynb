{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Steven Koprowicz, \n",
    " > Matthew Walters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=64, rate=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        # Use the rate variable for the dropout layers\n",
    "        \n",
    "        # MultiHeadAttention layer, \n",
    "        # specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(inputs,inputs, use_causal_mask=True)\n",
    "         \n",
    "        # drop-out layer\n",
    "        first_dropout = layers.Dropout(self.rate)(mha)\n",
    "        \n",
    "        # sum inputs and the output of this drop-out layer\n",
    "        first_sum = inputs + first_dropout\n",
    "\n",
    "        # LayerNormalization layer, \n",
    "        # specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        first_ln = layers.LayerNormalization(epsilon=1e-6)(first_sum)\n",
    "        \n",
    "        # dense\n",
    "        first_dense = layers.Dense(self.ff_dim, activation=\"relu\")(first_ln)\n",
    "        \n",
    "        # dense\n",
    "        second_dense = layers.Dense(self.embed_dim)(first_dense)\n",
    "\n",
    "        # drop-out layer\n",
    "        second_dropout = layers.Dropout(self.rate)(second_dense)\n",
    "\n",
    "        # sum the output of the first Layer Normalization layer and this drop-out layer\n",
    "        second_sum = first_ln + second_dropout\n",
    "\n",
    "        # LayerNormalization again\n",
    "        second_ln = layers.LayerNormalization(epsilon=1e-6)(second_sum)\n",
    "\n",
    "        return second_ln\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to encode positions\n",
    "        positions = tf.range(self.maxlen)\n",
    "        tokens_embed = layers.Embedding(self.vocab_size, self.embed_dim)\n",
    "        positions_embed = layers.Embedding(self.maxlen, self.embed_dim)\n",
    "        \n",
    "        #add (1) and (2) and return the layer\n",
    "        embedding_output = tokens_embed(inputs) + positions_embed(positions)\n",
    "        return embedding_output\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function\n",
    "        # (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "        inputs = layers.Input(shape=(self.maxlen,))\n",
    "        embedLayer = self.EmbeddingLayer(inputs)\n",
    "        xBlock = self.TransformerBlock(embedLayer)\n",
    "        for i in range(self.num_blocks-1):\n",
    "            xBlock = self.TransformerBlock(xBlock)\n",
    "        outputBlock = layers.Dense(self.vocab_size, activation=\"softmax\")(xBlock)\n",
    "        self.model = keras.Model(inputs, outputBlock)\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "227111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.filename=filename\n",
    "        self.len = len\n",
    "        with open(filename, \"r\") as f:\n",
    "            self.data = f.readlines()\n",
    "        \n",
    "    def prep_text(self):\n",
    "        #remove all punctuation, set to lowercase, remove duplicate spaces and other whitespace (keep newlines)\n",
    "        self.data = [re.sub(\"--\",\" \",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\n\",\" \\n$\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"((^|(?<=[^\\\\S\\n]))[^\\\\S\\n]+(?!$)|[^\\\\S\\n]+(?= \\\\n?$))\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\\\s+$\",\" \\n\",line) for line in self.data if (re.search(\"[A-Za-z0-9]\",line) is not None)]\n",
    "        return self.data\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number number\n",
    "        vocab_lines = [re.split(\" +\",line) for line in self.data]\n",
    "        self.words = []\n",
    "        for vl in vocab_lines:\n",
    "            self.words+=vl\n",
    "        vocab_list = np.unique(self.words)\n",
    "        self.vocabulary = {word:number for (word,number) in zip(vocab_list,range(len(vocab_list)))}\n",
    "        self.integers = [self.vocabulary[word] for word in self.words]\n",
    "        return self.integers\n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        if self.len==0 or self.len>len(self.integers):\n",
    "           return (self.x, self.y, self.vocabulary)\n",
    "        # this loop makes (#tokens - len + 1) sequences (1 is removed because of the offset)\n",
    "        for i in range(len(self.integers)-self.len+1):\n",
    "            self.x.append(self.integers[i:(i+self.len)])\n",
    "        self.y = self.x[1:]\n",
    "        self.x = self.x[:-1]\n",
    "        # this loop makes floor(#tokens / len) sequences\n",
    "        # for i in range((len(self.integers)-1)//self.len):\n",
    "        #     self.x.append(self.integers[(self.len*i):(self.len*(i+1))])\n",
    "        #     self.y.append(self.integers[(self.len*i+1):(self.len*(i+1)+1)])\n",
    "        return (self.x, self.y, self.vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        # vocab is a dictionary and thus already is our map from integers to human words\n",
    "        self.words_to_ints = vocab\n",
    "        self.ints_to_words = {v: k for k, v in vocab.items()}\n",
    "        self.vocabulary_list = vocab.keys()\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        text_out = []\n",
    "        text_out.append(start_string)\n",
    "        start_string = start_string\n",
    "        start_string = re.sub(\"--\",\" \",start_string)\n",
    "        start_string = re.sub(\"\\n\",\" \\n \",start_string)\n",
    "        start_string = re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",start_string)\n",
    "        start_string = re.sub(\"[^\\\\S\\n]+\",\" \",start_string)\n",
    "        start_string = re.sub(\"([^\\\\S\\n]+$|^[^\\\\S\\n]+)\",\"\",start_string)\n",
    "        output_string = start_string\n",
    "        \n",
    "        start_text_seq = re.split(\"[^\\\\S\\n]+\",start_string)\n",
    "        output_int_seq = [self.words_to_ints[word] if word in self.vocabulary_list else -1 for word in start_text_seq]\n",
    "        \n",
    "        for i in range(num_generate):\n",
    "            next_int_word = self.model.predict(output_int_seq)\n",
    "            output_string = output_string + \" \" + self.ints_to_words[next_int_word]\n",
    "\n",
    "        return(output_string)\n",
    "\n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        text_out = []\n",
    "        starter = self.vocabulary_list[np.random.randint(low=0,high=len(self.vocabulary_list))]\n",
    "        text_out = generate_text(starter, num_generate)\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 435ms/step - loss: 5.8815 - val_loss: 2.3387\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'query' (type EinsumDense).\n    \n    Shape must be rank 3 but is rank 2\n    \t for 0th input and equation: abc,cde->abde for '{{node model_12/multi_head_attention_12/query/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](model_12/tf.__operators__.add_36/AddV2, model_12/multi_head_attention_12/query/einsum/Einsum/ReadVariableOp)' with input shapes: [?,256], [256,2,256].\n    \n    Call arguments received by layer 'query' (type EinsumDense):\n      • inputs=tf.Tensor(shape=(None, 256), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m w \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     14\u001b[0m     x, y, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, validation_data\u001b[39m=\u001b[39m(x, y)\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m generator  \u001b[39m=\u001b[39m GenerateText(model,vocab_map)\n\u001b[1;32m---> 18\u001b[0m generator\u001b[39m.\u001b[39;49mgenerate_text(\u001b[39m\"\u001b[39;49m\u001b[39mhungry eyes\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[189], line 25\u001b[0m, in \u001b[0;36mGenerateText.generate_text\u001b[1;34m(self, start_string, num_generate)\u001b[0m\n\u001b[0;32m     22\u001b[0m output_int_seq \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords_to_ints[word] \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocabulary_list \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m start_text_seq]\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_generate):\n\u001b[1;32m---> 25\u001b[0m     next_int_word \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(output_int_seq)\n\u001b[0;32m     26\u001b[0m     output_string \u001b[39m=\u001b[39m output_string \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mints_to_words[next_int_word]\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m(output_string)\n",
      "File \u001b[1;32mc:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej7fr2f0b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\skoprowi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'query' (type EinsumDense).\n    \n    Shape must be rank 3 but is rank 2\n    \t for 0th input and equation: abc,cde->abde for '{{node model_12/multi_head_attention_12/query/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](model_12/tf.__operators__.add_36/AddV2, model_12/multi_head_attention_12/query/einsum/Einsum/ReadVariableOp)' with input shapes: [?,256], [256,2,256].\n    \n    Call arguments received by layer 'query' (type EinsumDense):\n      • inputs=tf.Tensor(shape=(None, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50):\n",
    "    return model    \n",
    "\n",
    "\n",
    "ds = DataSet(\"beatles.txt\",13)\n",
    "x,y,vocab_map = ds.create_dataset()\n",
    "tmodel_object = TransformerModel(len(vocab_map.keys()))\n",
    "x = keras.preprocessing.sequence.pad_sequences(x[1:100],tmodel_object.maxlen)\n",
    "y = keras.preprocessing.sequence.pad_sequences(y[1:100],tmodel_object.maxlen)\n",
    "model = tmodel_object.create_model()\n",
    "#print(model.summary())\n",
    "w = model.fit(\n",
    "    x, y, batch_size=32, epochs=1, validation_data=(x, y)\n",
    ")\n",
    "\n",
    "generator  = GenerateText(model,vocab_map)\n",
    "generator.generate_text(\"hungry eyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5537ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b21ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb59a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faceae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ddc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "##### In this project, we set out to construct a transformer model to generate text. The model is trained on the lyrics of Beatles songs. The song lyrics have been stripped of all punctuation, but keep individual spaces between words, and newlines are kept in the text so that the model might generate newlines in the text that it creates. In the end, the model should be able to generate rudimentary lyrics of fictitious Beatles music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "We used only the libraries provided at the top of this file, and our tf version is \n",
    "2.11.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
