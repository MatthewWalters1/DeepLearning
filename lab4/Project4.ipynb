{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Steven Koprowicz, \n",
    " > Matthew Walters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=64, rate=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        # Use the rate variable for the dropout layers\n",
    "        \n",
    "        # MultiHeadAttention layer, \n",
    "        # specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim, use_causal_mask=True)\n",
    "         \n",
    "        # drop-out layer\n",
    "        first_dropout = layers.Dropout(self.rate)(mha)\n",
    "        \n",
    "        # sum inputs and the output of this drop-out layer\n",
    "        first_sum = layers.Add()([inputs, first_dropout])\n",
    "\n",
    "        # LayerNormalization layer, \n",
    "        # specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        first_ln = layers.LayerNormalization(epsilon=1e-6)(first_sum)\n",
    "        \n",
    "        # dense\n",
    "        first_dense = layers.Dense(self.ff_dim, activation=\"relu\")(first_ln)\n",
    "\n",
    "        # dense\n",
    "        second_dense = layers.Dense(self.ff_dim, activation=\"relu\")(first_dense)\n",
    "\n",
    "        # drop-out layer\n",
    "        second_dropout = layers.Dropout(self.rate)(second_dense)\n",
    "\n",
    "        # sum the output of the first Layer Normalization layer and this drop-out layer\n",
    "        second_sum = layers.Add()([first_ln, second_dropout])\n",
    "\n",
    "        # LayerNormalization again\n",
    "        second_ln = layers.LayerNormalization(epsilon=1e-6)(second_sum)\n",
    "\n",
    "        output_dense = layers.Dense(self.ff_dim, activation=\"softmax\")(second_ln)\n",
    "\n",
    "        return output_dense\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to encode positions\n",
    "        positions = tf.range(len(inputs))\n",
    "        tokens = layers.Embedding(inputs, self.embed_dim)\n",
    "        posits = layers.Embedding(positions, self.embed_dim)\n",
    "        \n",
    "        #add (1) and (2) and return the layer\n",
    "        embedding_output = layers.Add()([tokens, posits])\n",
    "        return embedding_output\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, \n",
    "        # use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function\n",
    "        # (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "        embedLayer = EmbeddingLayer(self.vocab_size)\n",
    "        for i in range(self.num_blocks):\n",
    "            if i == 0:\n",
    "                xBlock = TransformerBlock(embedLayer)\n",
    "            else:\n",
    "                xBlock = TransformerBlock(xBlock)\n",
    "        return xBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "227111a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38912\n",
      "38911\n",
      "38911\n",
      "[[13, 510, 1069, 2222, 1229, 0, 1050, 1753, 2222, 1496, 2276, 1549, 262], [510, 1069, 2222, 1229, 0, 1050, 1753, 2222, 1496, 2276, 1549, 262, 0]]\n",
      "[[510, 1069, 2222, 1229, 0, 1050, 1753, 2222, 1496, 2276, 1549, 262, 0], [1069, 2222, 1229, 0, 1050, 1753, 2222, 1496, 2276, 1549, 262, 0, 18]]\n",
      "[[1441, 1954, 1170, 2579, 1441, 1954, 0, 2579, 1441, 1954, 1170, 2559, 0], [1954, 1170, 2579, 1441, 1954, 0, 2579, 1441, 1954, 1170, 2559, 0, 133], [1170, 2579, 1441, 1954, 0, 2579, 1441, 1954, 1170, 2559, 0, 133, 2275]]\n",
      "[[1954, 1170, 2579, 1441, 1954, 0, 2579, 1441, 1954, 1170, 2559, 0, 133], [1170, 2579, 1441, 1954, 0, 2579, 1441, 1954, 1170, 2559, 0, 133, 2275], [2579, 1441, 1954, 0, 2579, 1441, 1954, 1170, 2559, 0, 133, 2275, 0]]\n"
     ]
    }
   ],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.filename=filename\n",
    "        self.len = len\n",
    "        with open(filename, \"r\") as f:\n",
    "            self.data = f.readlines()\n",
    "        \n",
    "    def prep_text(self):\n",
    "        #remove all punctuation, set to lowercase, remove duplicate spaces and other whitespace (keep newlines)\n",
    "        self.data = [re.sub(\"--\",\" \",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\n\",\" \\n$\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"((^|(?<=[^\\\\S\\n]))[^\\\\S\\n]++(?!$)|[^\\\\S\\n]+(?= \\\\n?$))\",\"\",line) for line in self.data]\n",
    "        self.data = [re.sub(\"\\\\s++$\",\" \\n\",line) for line in self.data if (re.search(\"[A-Za-z0-9]\",line) is not None)]\n",
    "        return self.data\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number number\n",
    "        vocab_lines = [re.split(\" +\",line) for line in self.data]\n",
    "        self.words = []\n",
    "        for vl in vocab_lines:\n",
    "            self.words+=vl\n",
    "        vocab_list = np.unique(self.words)\n",
    "        self.vocabulary = {word:number for (word,number) in zip(vocab_list,range(len(vocab_list)))}\n",
    "        self.integers = [self.vocabulary[word] for word in self.words]\n",
    "        return self.integers\n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        if self.len==0 or self.len>len(self.integers):\n",
    "           return (self.x, self.y, self.vocabulary)\n",
    "        # this loop makes (#tokens - len + 1) sequences (1 is removed because of the offset)\n",
    "        for i in range(len(self.integers)-self.len+1):\n",
    "            self.x.append(self.integers[i:(i+self.len)])\n",
    "        self.y = self.x[1:]\n",
    "        self.x = self.x[:-1]\n",
    "        # this loop makes floor(#tokens / len) sequences\n",
    "        # for i in range((len(self.integers)-1)//self.len):\n",
    "        #     self.x.append(self.integers[(self.len*i):(self.len*(i+1))])\n",
    "        #     self.y.append(self.integers[(self.len*i+1):(self.len*(i+1)+1)])\n",
    "        return (self.x, self.y, self.vocabulary)\n",
    "\n",
    "ds = DataSet(\"beatles.txt\",13)\n",
    "x,y,v = ds.create_dataset()\n",
    "print(ds.x[0:2])\n",
    "print(ds.y[0:2])\n",
    "print(ds.x[-3:])\n",
    "print(ds.y[-3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'ddnudh', '\\n', 'dthe', 're', 'fred']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[39mreturn\u001b[39;00m text_out\n\u001b[0;32m     31\u001b[0m x \u001b[39m=\u001b[39m GenerateText(\u001b[39m1\u001b[39m,{\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m2\u001b[39m})\n\u001b[1;32m---> 32\u001b[0m x\u001b[39m.\u001b[39;49mgenerate_text(\u001b[39m\"\u001b[39;49m\u001b[39m     \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m ddnu\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdh\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m dthe--re fred ! \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[169], line 20\u001b[0m, in \u001b[0;36mGenerateText.generate_text\u001b[1;34m(self, start_string, num_generate)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(re\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mS\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m]+\u001b[39m\u001b[39m\"\u001b[39m,start_string))\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_generate):\n\u001b[1;32m---> 20\u001b[0m     thisWord \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(lastWord)\n\u001b[0;32m     21\u001b[0m     text_out\u001b[39m.\u001b[39mappend(thisWord)\n\u001b[0;32m     22\u001b[0m     lastWord \u001b[39m=\u001b[39m thisWord\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        # vocab is a dictionary and thus already is our map from integers to human words\n",
    "        self.vocabulary_map = vocab\n",
    "        self.vocabulary_list = vocab.keys()\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        text_out = []\n",
    "        text_out.append(start_string)\n",
    "        start_string = start_string\n",
    "        start_string = re.sub(\"--\",\" \",start_string)\n",
    "        start_string = re.sub(\"\\n\",\" \\n \",start_string)\n",
    "        start_string = re.sub(\"[^A-Za-z0-9\\\\s]\",\"\",start_string)\n",
    "        start_string = re.sub(\"[^\\\\S\\n]++\",\" \",start_string)\n",
    "        start_string = re.sub(\"([^\\\\S\\n]++$|^[^\\\\S\\n]++)\",\"\",start_string)\n",
    "        start_seq = re.split(\"[^\\\\S\\n]+\",start_string)\n",
    "        output_seq = start_seq[::]\n",
    "        for i in range(num_generate):\n",
    "            thisWord = self.model.predict(output_seq)\n",
    "            text_out.append(thisWord)\n",
    "            output_seq.append(thisWord)\n",
    "        return text_out\n",
    "\n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        text_out = []\n",
    "        starter = self.vocabulary_list[np.random.randint(low=0,high=len(self.vocabulary_list))]\n",
    "        text_out = generate_text(starter, num_generate)\n",
    "        return text_out\n",
    "\n",
    "x = GenerateText(1,{\"a\":2})\n",
    "x.generate_text(\"     \\n ddnu'dh\\n dthe--re fred ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50):\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5537ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b21ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb59a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faceae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ddc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "##### In this project, we set out to construct a transformer model to generate text. The model is trained on the lyrics of Beatles songs. The song lyrics have been stripped of all punctuation, but keep individual spaces between words, and newlines are kept in the text so that the model might generate newlines in the text that it creates. In the end, the model should be able to generate rudimentary lyrics of fictitious Beatles music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "We used only the libraries provided at the top of this file, and our tf version is \n",
    "2.11.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
